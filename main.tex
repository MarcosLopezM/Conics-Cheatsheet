\documentclass{article}

\usepackage{amsfonts, amsmath,amssymb,amsthm}   % Paquetes de simbología matemática básica
\usepackage{lmodern,microtype,bm}     % Fuente y espaciado entre letras; lo deja bonito
\usepackage{dsfont, graphicx}
\usepackage{mathrsfs, halloweenmath,xcolor}
\usepackage{MnSymbol}
\usepackage{mathtools}
\usepackage{multicol,titlesec}
\usepackage[shortlabels]{enumitem}    % Continuar listas en mini-páginas distintas
\usepackage{physics}
\usepackage[english,spanish]{babel}   % Cambia los comandos de texto predeterminados (capítulos, 			                                        secciones, bibliografía, etc.) a español
\decimalpoint
\usepackage[style=mexican]{csquotes}  % Comillas y otros elementos de citación
\textwidth 16cm                       % Ancho
\oddsidemargin -0.0cm                 % Espacio de margen (como es formato de libro, los margenes se 		                                        declaran para páginas pares e impares

\newtheoremstyle{definicion}% name
{3pt}% Space above
{3pt}% Space below
{}% Body font
{}% Indent amount
{\color{blue}\bfseries}% Theorem head font
{.}% Punctuation after theorem head
{.5em}% Space after theorem head
{}%
\theoremstyle{definicion}
\newtheorem{definicion}{Def.}

\theoremstyle{definition}             % Con el paquete amsmath se pueden personalizar los estilos de
\newtheorem*{inst}{Instrucciones}

\theoremstyle{definition}             % Con el paquete amsmath se pueden personalizar los estilos de
\newtheorem{sol}{Solución}

\theoremstyle{definition}
\newtheorem{record}{Recordatorio}

\theoremstyle{definition}
\newtheorem{properties}{Propiedades}

\newtheoremstyle{observacion}% name
{3pt}% Space above
{3pt}% Space below
{}% Body font
{}% Indent amount
{\color{red}\bfseries}% Theorem head font
{.}% Punctuation after theorem head
{.5em}% Space after theorem head
{}%
\theoremstyle{observacion}
\newtheorem{obs}{Obs.}

\theoremstyle{definition}
\newtheorem{prop}{Proposición}

\theoremstyle{plain}
\newtheorem{lemma}{Lema}
\newtheorem{theorem}{Teorema}

\theoremstyle{definition}
\newtheorem{exe}{Ejemplo}

\newtheoremstyle{afirmacion}% name
{3pt}% Space above
{3pt}% Space below
{}% Body font
{}% Indent amount
{\color{green!40!black}\bfseries}% Theorem head font
{.}% Punctuation after theorem head
{.5em}% Space after theorem head
{}%
\theoremstyle{afirmacion}
\newtheorem{corollary}{Corolario}

\newtheoremstyle{notation}% name
{3pt}% Space above
{3pt}% Space below
{}% Body font
{}% Indent amount
{\color{magenta}\bfseries}% Theorem head font
{.}% Punctuation after theorem head
{.5em}% Space after theorem head
{}%
\theoremstyle{notation}
\newtheorem{notation}{Notación}

\theoremstyle{definition}
\newtheorem{eje}{Ejercicio}

\setlength{\parindent}{2em}           % Sangría
\setlength{\parskip}{0.5em}           % Espacio entre párrafos

\title{\Huge{Funciones lineales}}
\author{Geometría Analítica I}
\date{\today}

\begin{document}
    \maketitle

    Hemos visto que las transformaciones ortogonales de \(\mathbb{R}^{2}\) quedan determinadas por lo que le hacen a la base canónica; es decir, que se escriben

    \begin{equation*}
        f(x, y) = x\vb*{u} + y\vb*{v},
    \end{equation*}

    donde \(\vb*{u}\) y \(\vb*{v}\) son vectores fijos (\(\vb*{u} = f(\vb*{e}_{1})\) y \(\vb*{v} = f(\vb*{e}_{2})\)); y además, vimos que si la función es ortogonal entonces \(\vb*{u}\) y \(\vb*{v}\) \textcolor{red}{forman una base ortonormal}. Pero si en esta fórmula no pedimos nada a \(\vb*{u}\) y a \(\vb*{v}\), simplemente que sean vectores cualesquiera en \(\mathbb{R}^{2}\), obtenemos una familia de funciones mucho más grande: las \textcolor{blue}{lineales} de \(\mathbb{R}^{2}\) en \(\mathbb{R}^{2}\).

    \begin{definicion}
        Una función \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) es \textcolor{blue}{lineal} si para todos los vectores \(x,\ y\in \mathbb{R}^{n}\) y todo número \(t\in\mathbb{R}\) se cumple que:

        \begin{enumerate}[label = \Roman*)]
            \item \(f(x + y) = f(x) + f(y)\)
            \item \(f(tx) = tf(x)\).
        \end{enumerate}
    \end{definicion}

    \begin{theorem}
        Una función \(f \colon \mathbb{R}^{2} \to \mathbb{R}^{2}\) es lineal si y solo si se escribe

        \begin{equation*}
            f(x, y) = x\vb*{u} + y\vb*{v},
        \end{equation*}

        donde \(\vb*{u}\) y \(\vb*{v}\) son vectores fijos en \(\mathbb{R}^{2}\).
    \end{theorem}

    \begin{obs}
        Las transformaciones ortogonales son funciones lineales.
    \end{obs}

    \section{Extensión lineal}

    Dados los vectores \(\vb*{v}_{1},\ \vb*{v}_{2}, \dots, \vb*{v}_{k}\) en \(\mathbb{R}^{n}\), una \textcolor{blue}{combinación lineal} de ellos es el vector

    \begin{equation*}
        \sum_{i = 1}^{k} \lambda_{i} \vb*{v}_{i}\in\mathbb{R}^{n},
    \end{equation*}

    donde \(\lambda_{i}\in\mathbb{R},\ i = 1, \dots, k\), son escalares cualesquiera a los que llamamos los \textcolor{red}{coeficientes} de la combinación lineal.

    Si tenemos una función lineal \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\), se cumple que preserva combinaciones lineales pues

    \begin{equation*}
        f\left(\sum_{i = 1}^{k} \lambda_{i} \vb*{v}_{i}\right) = \sum_{i = 1}^{k} f(\lambda_{i} \vb*{v}_{i}) = \sum_{i = 1}^{k} \lambda_{i} f(\vb*{v}_{i}).
    \end{equation*}
    
    Así que podemos redefinir la \textcolor{blue}{función lineal} como aquella que cumple:
    
    \begin{equation*}
        f\left(\sum_{i = 1}^{k} \lambda_{i} \vb*{v}_{i}\right) = \sum_{i = 1}^{k} \lambda_{i} f(\vb*{v}_{i}),
    \end{equation*}

    para cualesquiera \(\vb*{v}_{1},\ \vb*{v}_{2}, \dots, \vb*{v}_{k}\in\mathbb{R}^{n}\) y \(\lambda_{1},\ \lambda_{2}, \dots,\ \lambda_{k}\in\mathbb{R}\).

    Si tomamos \(n\) vectores \(\vb*{u}_{1},\ \vb*{u}_{2}, \dots,\ \vb*{u}_{n} \in \mathbb{R}^{m}\) arbitrariamente, y queremos encontrar una función lineal \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) que cumpla \(f(\vb*{e}_{i}) = \vb*{u}_{i}\) para \(i = 1, 2, \dots, n\), entonces debemos definirla en todo \(\mathbb{R}^{n}\) como

    \begin{equation*}
        f(x_{1}, x_{2}, \dots, x_{n}) = \sum_{i = 1}^{n} x_{i} \vb*{u}_{i}.
    \end{equation*}

    A esta función se le llama la \textcolor{red}{extensión lineal a \(\mathbb{R}^{n}\)} de lo que determinamos para la base canónica.

    \begin{theorem}
        Una función \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) es lineal si y solo si se escribe

        \begin{equation*}
            f(x_{1}, x_{2}, \dots, x_{n}) = x_{1}\vb*{u}_{1} + x_{2}\vb*{u}_{2} + \cdots + x_{n}\vb*{u}_{n} = \sum_{i = 1}^{n} x_{i} \vb*{u}_{i},
        \end{equation*}

        donde \(\vb*{u}_{1},\ \vb*{u}_{2},\ \dots,\ \vb*{u}_{n}\in\mathbb{R}^{m}\).
    \end{theorem}

    \begin{obs}
        \(\vb*{u}_{i} = f(\vb*{e}_{i}).\)
    \end{obs}

    \begin{corollary}
        Si \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) y \(g \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) son dos funciones lineales tales que \(f(\vb*{e}_{i}) = g(\vb*{e}_{i})\) para \(i = 1,\ 2,\ \dots,\ n\), entonces \(f(x) = g(x)\) para toda \(x\in\mathbb{R}^{n}\).
    \end{corollary}

    \section{La estructura de las funciones lineales}

    Sea  \(\mathcal{L}(n, m)\) el conjunto de todas las funciones lineales de \(\mathbb{R}^{n} \to \mathbb{R}^{m}\), es decir,

    \begin{equation*}
        \mathcal{L}(n, m) = \left\lbrace f \colon \mathbb{R}^{n} \to \mathbb{R}^{m} \mid \text{f es lineal}\right\rbrace.
    \end{equation*}

    Con el codominio se tienen las operaciones de suma y multiplicación por escalares, estas operaciones se pueden definir también en las funciones; en cierta, las funciones las heredan. Dadas \(f,\ g \in \mathcal{L}(n, m)\) y \(t\in\mathbb{R}\), sean

    \begin{equation*}
        f + g \colon \mathbb{R}^{n} \to \mathbb{R}^{m}, \qquad (f + g)(x) \coloneq f(x) + g(x)
    \end{equation*}

    y

    \begin{equation*}
        tf \ordinarycolon \mathbb{R}^{n} \to \mathbb{R}^{m}, \qquad (tf)(x) \coloneq t(f(x)).
    \end{equation*}

    \section{Matrices}

    Las matrices son arreglos rectangulares de números, tablas podría decirse, donde los renglones y las columnas no tienen ningún significado extra. En nuestro contexto actual, habrán de ser los \emph{paquetes} que cargan toda la información de las funciones lineales y nos darán la herramienta para hacer cálculos respecto de ellas, además de simplificar la notación.

    \subsection{Vectores columna}

    \begin{notation}
        De ahora en adelante, los vectores en \(\mathbb{R}^{n}\) se escribirán como columnas y no como renglones. Es decir, si antes escribíamos \(x = (x_{1}, x_{2}, \dots, x_{n})\), ahora escribiremos

        \begin{equation*}
            x = \mqty(x_{1}\\ x_{2}\\ \vdots \\ x_{n}).
        \end{equation*}

        El inconveniente de que entonces se complica escribir un vector dentro del texto, lo subsanamos al llamar a los vectores renglón \textcolor{blue}{transpuestos}, y los denotaremos con \(x^{\top} = (x_{1}, x_{2}, \dots, x_{n})\), o bien, \(x = (x_{1}, x_{2}, \dots, x_{n})^{\top}\), conviniendo que los \textcolor{blue}{transpuestos} de vectores renglón son los vectores. Aunque a veces se nos va a olvidar poner el exponente \(\top\) de ``transpuesto''.

        Así, por ejemplo, tenemos que la base canónica de \(\mathbb{R}^{2}\) es \(\vb*{e}_{1} = (1, 0)^{\top}\) y \(\vb*{e}_{2} = (0, 1)^{\top}\), que quiere decir que

        \begin{equation*}
            \vb*{e}_{1} = \mqty(1 \\ 0) \quad \text{y} \quad \vb*{e}_{2} = \mqty(0 \\ 1).
        \end{equation*}
    \end{notation}

    \subsection{La matriz de una función lineal}

    Una matriz de \(m \cross n\) es un arreglo rectangular (o tabla) de números reales con \(m\) renglones y \(n\) columnas. Si, usando dos subíndices, denotamos con \(a_{ij}\) al número que están en el renglón \(i\) y la columna \(j\), llamado \textcolor{blue}{entrada}, tenemos que una matriz de \(m \cross n\) es

    \begin{equation*}
        A = \mqty(a_{11} & a_{12} & \cdots & a_{1n}\\
        a_{21} & a_{22} & \cdots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{m1} & a_{m2} & \cdots & a_{mn}).
    \end{equation*}

    Podríamos también definirla como un conjunto ordenado de \(n\) vectores en \(\mathbb{R}^{m}\): sus columnas. Es decir, la matriz \(A\) anterior se puede escribir como 

    \begin{equation*}
        A = (\vb*{u}_{1}, \vb*{u}_{2}, \dots, \vb*{u}_{n}), \; \text{donde}\; \vb*{u}_{i} = \mqty(a_{1i}\\ a_{2i}\\ \vdots\\ a_{mi})\in\mathbb{R}^{m},\ i = 1, 2, \dots, n.
    \end{equation*}

    \begin{obs}
        A la matriz \(A\) se le asocia la función lineal \(f \colon \mathbb{R}^{n}\to \mathbb{R}^{m}\) que manda al vector canónico \(\vb*{e}_{i} \in \mathbb{R}^{n}\) en su columna i-ésima, es decir, tal que \(f(\vb*{e}_{i}) = \vb*{u}_{i}\) para \(i = 1, 2, \dots, n\).
    \end{obs}

    A cada función lineal \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\)  se le asocia la matriz \(m \cross n\) que tiene como columnas a sus valores en la base canónica, es decir, se le asocia la matriz

    \begin{equation*}
        A = (f(\vb*{e}_{1}), f(\vb*{e}_{2}), \dots, f(\vb*{e}_{n})).
    \end{equation*}

    \begin{exe}
        A la función lineal de \(\mathbb{R}^{3}\) en \(\mathbb{R}^{3}\) se le asocia la matriz

        \begin{equation*}
            \mqty(1 & 6 & 2\\
                2 & 5 & 3\\
                3 & 4 & 1).
        \end{equation*}
    \end{exe}

    \begin{exe}
        A la transformación ``compadre ortogonal'' de \(\mathbb{R}^{2}\) en \(\mathbb{R}^{2}\) se le asocia la matriz
        
        \begin{equation*}
            \mqty(1 & 0\\
                0 & 1).
        \end{equation*}
    \end{exe}

    Definamos la el \textcolor{red}{producto de una matriz por un vector} para que el resultado sea lo que su función lineal asociada hace al vector. Es decir, si \(A\) es una matriz \(m \cross n\), podrá multiplicar solo a vectores \(x \in \mathbb{R}^{n}\) y el resultado será un vector en \(\mathbb{R}^{m}\). Como ya vimos, \(A\) se puede escribir como \(A = (\vb*{u}_{1}, \vb*{u}_{2}, \dots, \vb*{u}_{n})\), donde \(\vb*{u}_{i} \in \mathbb{R}^{m}\), para \(i = 1, 2, \dots, n\); y por su parte, \(x = (x_{1}, x_{2}, \dots, x_{n})^{\top}\). Definamos entonces el \emph{producto} de \(A\) por \(x\) como

    \begin{equation*}
        A x = (\vb*{u}_{1}, \vb*{u}_{2}, \dots, \vb*{u}_{n})\mqty(x_{1} \\ x_{2} \\ \vdots \\ x_{n}) \coloneq x_{1}\vb*{u}_{1} + x_{2}\vb*{u}_{2} + \cdots + x_{n}\vb*{u}_{n}.
    \end{equation*}

    \begin{theorem}
        Las funciones lineales de \(\mathbb{R}^{n}\) en \(\mathbb{R}^{m}\) están en correspondencia natural y biunívoca con las matrices de \(m \cross n\), de tal manera que a la función \(f\) le corresponde la matriz \(A\) que cumple

        \begin{equation*}
            f(x) = Ax
        \end{equation*}

        para todo \(x \in \mathbb{R}^{n}\).
    \end{theorem}

    \begin{obs}
        Cuando \(m = 1\) el producto que acabamos de definir corresponde al producto interior. Es decir, si 

        \begin{equation*}
            y^{\top} x = y \vdot x.
        \end{equation*}
    \end{obs}

    \subsection{Multiplicación de matrices}

    Vamos ahora a definir la multiplicación de matrices correspondiendo a la composición de funciones lineales.

    Sean \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{m}\) y \(g \colon \mathbb{R}^{m} \to \mathbb{R}^{k}\), entonces \(g \circ f \colon \mathbb{R}^{n} \to \mathbb{R}^{k}\) también es lineal. Sean \(A\) la matriz \(m \cross n\) y \(B\) la matriz \(k \cross m\) que corresponden a \(f\) y a \(g\) respectivamente. Definimos el producto \(BA\) como la matriz \(k \cross n\) que corresponde a la función lineal \(g \circ f\) . Es decir, \(BA\) es la única matriz \(k \cross n\) que cumple

    \begin{equation*}
        (g \circ f)(x) = (BA)x \quad \text{para todo} \quad x \in \mathbb{R}^{n}.
    \end{equation*}

    \subsection{Algunas familias distinguidas de matrices}

    \subsubsection*{La matriz identidad}

    La \textcolor{blue}{matriz identidad}, o simplemente ``la identidad'' , es la matriz asociada a la función identidad, que se denota como \(I\), y es la matriz que tiene \(1\) en la diagonal y \(0\) fuera de ella, es decir,

    \begin{equation*}
        I = \mqty(1 & 0 & 0 & \cdots & 0\\
            0 & 1 & 0 & \cdots & 0\\
            0 & 0 & 1 & \cdots & 0\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & \cdots & 1).
    \end{equation*}

    \begin{obs}
        La matriz identidad cumple que \(A I = A\) e \(I A = A\) para cualquier matriz \(A\) que se deje multiplicar por ella: es la \textcolor{blue}{identidad multiplicativa}.
    \end{obs}

    \subsubsection*{Homotecias}

    Las \textcolor{blue}{homotecias}, como funciones, son simples ``cambios de escala''. Si tomamos un número \(k \in \mathbb{R}\), con \(k \neq 0\), la función \(f \colon \mathbb{R}^{n} \to \mathbb{R}^{n}\) definida por \(f(x) = kx\) es claramente lineal y se llama una \textcolor{blue}{homotecia}. Entonces tiene una matriz asociada que es \(kI\), la matriz que tiene solo \(k\) en la diagonal y \(0\) fuera de ella. Como función, lo que hace es expandir uniformemente desde el origen (si \(k > 1\)), o bien contraer (cuando \(k < 1\)).

    \begin{obs}
        Las homotecias tienen la propiedad de conmutar con cualquier otra matriz, pues \(A(kI) = k(AI) = kA = k(IA) = (kI)A\).
    \end{obs}

    \subsubsection*{Matrices de permutaciones}

    Recordemos que una permutación de \(n\) elementos es una función biyectiva \(\rho \colon \Delta_{n} \to \Delta_{n}\), donde podemos tomar \(\Delta_{n} = \left\lbrace 1, 2, \dots, n\right\rbrace\); y que todas estas permutaciones forman el grupo simétrico de orden \(n\), \(S_{n}\), con \(n!\) elementos. Ahora bien, a cada permutación \(\rho \colon \Delta_{n} \to \Delta_{n}\) se le puede asociar la función lineal \(f_{\rho} \colon \mathbb{R}^{n} \to \mathbb{R}^{n}\) definida por \(f_{\rho}(\vb*{e}_{i}) = \vb*{e}_{\rho(i)}\); es decir, \(f_{\rho}\) permuta a los elementos de la base canónica según \(\rho\) y se extiende linealmente a \(\mathbb{R}^{n}\). A la matriz asociada a esta función lineal se le llama la \textcolor{blue}{matriz de la permutación \(\rho\)}; a la que denotaremos como \(A_{\rho}\). 

    \begin{obs}
        La multiplicación de matrices de permutaciones es de nuevo una matriz de permutación y corresponde a la composición de las permutaciones correspondientes. Es decir, si \(\rho,\ \sigma \in S_{n}\), entonces \(A_{\rho}A_{\sigma} = A_{\rho \circ \sigma}\).
    \end{obs}

    \begin{obs}
        El grupo simétrico se puede ver como un ``grupo de matrices''.
    \end{obs}

    \subsubsection*{Rotaciones}

    Sabemos que una rotación es una transformación lineal que manda al vector canónico \(\vb*{e}_{1}\) en un vector unitario \(\vb*{u} \in \mathbb{S}^{1}\) y al otro vector canónico \(\vb*{e}_{2}\) en su ``compadre ortogonal'' \(\vb*{u}^{\top}\). Si escribimos a \(\vb*{u}\) en términos de su ángulo respecto del eje \(x\), es decir, respecto de \(\vb*{e}_{1}\), entonces obtenemos que la matriz asociada a la rotación por un ángulo \(\theta\) es

    \begin{equation*}
        R_{\theta} \coloneq \mqty(\cos\theta & -\sin \theta \\ \sin\theta & \cos\theta).
    \end{equation*}

    \begin{obs}
        \(R_{-\theta}R_{\theta} = I\).
    \end{obs}

    \begin{obs}
        \(R_{\alpha}R_{\beta} = R_{\alpha + \beta}\).
    \end{obs}

    \subsubsection*{Reflexiones}

    Conviene parametrizar las reflexiones no por el ángulo de la imagen de \(\vb*{e}_{1}\), sino por su mitad, que es el ángulo de la recta-espejo de la reflexión. Pues si, como es natural, a una recta por el origen le asociamos su ángulo (entre \(0\) y \(\pi\)) con el rayo positivo del eje \(x\), entonces la reflexión, \(E_{\theta}\), en la recta con ángulo \(\theta\) manda a \(\vb*{e}_{1}\) en el vector unitario de ángulo \(2\theta\), y por lo tanto la matriz asociada es
    
    \begin{equation*}
        E_{\theta} \coloneq \mqty(\cos 2\theta & \sin 2\theta\\ \sin 2\theta & -\cos 2\theta),
    \end{equation*}

    pues el segundo vector no es el ``compadre ortogonal'' sino su negativo.

    \begin{obs}
        Una reflexión es su propia inversa, \emph{i.e.}, \(E_{\theta}E_{\theta} = I\).
    \end{obs}

    \begin{obs}
        La composición de dos reflexiones es la rotación del doble del ángulo entre sus espejos, \emph{i.e.}, \(E_{\alpha}E_{\beta} = R_{2(\alpha - \beta)}\).
    \end{obs}

    \subsubsection*{Matrices ortogonales (y transpuestas)}

    Decimos que una matriz es \textcolor{red}{ortogonal} si su función lineal asociada es ortogonal. Entonces, podemos considera el grupo de transformaciones \(\vb*{O}(2)\) como un grupo de matrices con la operación de multiplicación.

    Llamemos a una matriz \textcolor{blue}{ortogonal} si es cuadrada, y su función lineal asociada es una transformación ortogonal (que preserva el producto interior). Entonces, si \(A\) es ortogonal, todas sus columnas son vectores unitarios y además por parejas son ortogonales. Dicho más explícitamente, si \(A = (\vb*{u}_{1}, \vb*{u}_{2}, \dots, \vb*{u}_{n})\) entonces \(\vb*{u}_{i} )f(\vb*{e}_{i})\) donde \(f\) es su función asociada (\(f(x) = Ax\)), y si \(f \in \vb*{O}(2)\) se cumple que

    \begin{equation*}
        \vb*{u}_{i} \vdot \vb*{u}_{j} = \vb*{e}_{i} \vdot \vb*{e}_{j} = \delta_{ij} \coloneq \begin{cases}
            1 & \text{si } i = j \\
            0 & \text{si } i \neq j,
        \end{cases}
    \end{equation*}

    donde \(\delta_{ij}\) es conocida como la \textcolor{blue}{delta de Kroenecker}. En este caso al conjunto ordenado de vectores \(\vb*{u}_{1}, \vb*{u}_{2}, \dots, \vb*{u}_{n}\) se le llama \textcolor{red}{base ortonormal}. Pero nótese que estos \(n^{2}\) productos interiores son las entradas de la matriz

    \begin{equation*}
        \mqty({\vb*{u}_{1}}^{\top}\\ {\vb*{u}_{2}}^{\top}\\ \vdots\\ {\vb*{u}_{n}}^{\top})(\vb*{u}_{1}, \vb*{u}_{2}, \cdots, \vb*{u}_{n}) = \mqty(u_{1} \vdot \vb*{u}_{1} & u_{1} \vdot \vb*{u}_{2} & \cdots & u_{1} \vdot \vb*{u}_{n}\\
        u_{2} \vdot \vb*{u}_{1} & u_{2} \vdot \vb*{u}_{2} & \cdots & u_{2} \vdot \vb*{u}_{n}\\
        \vdots & \vdots & \ddots & \vdots\\
        u_{n} \vdot \vb*{u}_{1} & u_{n} \vdot \vb*{u}_{2} & \cdots & u_{n} \vdot \vb*{u}_{n}). 
    \end{equation*}

    \begin{obs}
        Si llamamos \textcolor{red}{transpuesta} de una matriz a la que se obtiene cambiando columnas por renglones, hemos demostrado que la ``inversa'' de una matriz ortogonal es su transpuesta.
    \end{obs}

    En general, llamemos la \textcolor{blue}{transpuesta de una matriz}

    \begin{equation*}
        A = (\vb*{v}_{1}, \vb*{v}_{2}, \dots, \vb*{v}_{n})
    \end{equation*}

    a la matriz

    \begin{equation*}
        A^{\top} = \mqty(v_{1}^{\top}\\ v_{2}^{\top}\\ \vdots\\ v_{n}^{\top}),
    \end{equation*}

    que se obtiene al reflejar las entradas de la diagonal; si \(A\) es \(m \cross n\) entonces \(A^{\top}\) es \(n \cross m\).

    \begin{theorem}
        Una matriz \(A\) de \(n \cross n\) es ortogonal (la matriz de una transformación ortogonal) si y solo si

        \begin{equation*}
            A^{\top}A = I.
        \end{equation*}
    \end{theorem}

    \subsection{El Grupo General Lineal (GL(2))}

    \begin{definicion}
        Una matriz \(A\) de \(n \cross n\) es \textcolor{blue}{invertible} si existe otra matriz de \(n \cross n\), llamada su \textcolor{blue}{inversa} y denotada \(A^{-1}\), tal que \(A A^{-1} = A^{-1}A = I\). Al conjunto de todas las matrices invertibles de \(n \cross n\) se le llama el \textcolor{red}{grupo general lineal de orden n} y se le denota \(\bm{GL}(n)\). Así que \(A \in \bm{GL}(n)\) quiere decir que \(A\) es invertible yde \(n \cross n\).
    \end{definicion}

    \begin{obs}
        A \(\bm{GL}(2)\) se le puede pensar como grupo de transformaciones, o bien como un grupo de matrices.
    \end{obs}
    \begin{obs}
        La operación del grupo es la multiplicación de matrices.
    \end{obs}
    \begin{obs}
        \(\bm{O}(n)\) es un subgrupo de \(\bm{GL}(n)\).
    \end{obs}

    \subsubsection{El determinante}

    \begin{definicion}
        Definimos el \textcolor{blue}{determinante} de la matriz \(A\) como el número

        \begin{equation*}
            \det(
                \mqty{
                    a & b \\
                    c & d
                }
            ) \coloneq ad - bc.
        \end{equation*}

        Nótese que si \(A = (\vb*{u}, \vb*{v})\), entonces \(\det(A) = \det(\vb*{u}, \vb*{v})\).
    \end{definicion}
\end{document}